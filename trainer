
import yaml
import tensorflow as tf
import numpy as np
import torch
import os
# Please add the downloaded mmfi directory into your python project. 
from mmfi1 import make_dataset, make_dataloader, MMFi_Dataset, decode_config, MMFi_Database
import torch.nn as nn
from evaluation import compute_pck_pckh
from sklearn.model_selection import train_test_split
from evaluate import compute_similarity_transform, calulate_error
from SK_VQ_GAN import CSI, Discriminator


import thop 

#X = torch.rand(32,3,114,10)
embedding_dim = 32  # Embedding dimension, you can adjust this as needed
commitment_cost = 1 
model = CSI(embedding_dim,commitment_cost)
dis = Discriminator()
#metafi = metafi.cuda()
#flops, params = thop.profile(metafi, inputs=(X,))
#print(f"FLOPs: {flops / 1e9} GigaFLOPs")  # Chuyển đổi sang GigaFLOPs (tỷ lệ FLOPs)
#print(f"Parameters: {params / 1e6} Million")

dataset_root ='/home/toangian/Documents/Dataset1'
with open('config1.yaml', 'r') as fd:  # change the .yaml file in your code.
    config = yaml.load(fd, Loader=yaml.FullLoader)


train_dataset, test_dataset = make_dataset(dataset_root, config)
rng_generator = torch.manual_seed(config['init_rand_seed'])
train_loader = make_dataloader(train_dataset, is_training=True, generator=rng_generator, **config['train_loader'])
#testing_loader = make_dataloader(test_dataset, is_training=False, generator=rng_generator, **config['test_loader'])
val_data , test_data = train_test_split(test_dataset, test_size=0.5, random_state=41)
val_loader = make_dataloader(val_data, is_training=False, generator=rng_generator, **config['val_loader'])
test_loader = make_dataloader(test_data, is_training=False, generator=rng_generator, **config['test_loader'])


#metafi.apply(weights_init)
model = model.cuda()
dis = dis.cuda()

#l2_loss = nn.L2Loss().cuda() 
criterion1 = nn.BCELoss().cuda()
criterion2 = nn.MSELoss().cuda()
optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)
optimizer_GAN = torch.optim.Adam(dis.parameters(), lr = 0.001)
n_epochs = 20
n_epochs_decay = 30
epoch_count = 1
def lambda_rule(epoch):

    lr_l = 1.0 - max(0, epoch + epoch_count - n_epochs) / float(n_epochs_decay + 1)
    return lr_l
scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: 1.0 - max(0, epoch + epoch_count - n_epochs) / float(n_epochs_decay + 1))
#scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: 1.0 - max(0, epoch + epoch_count - n_epochs) / float(n_epochs_decay + 1))

l2_lambda = 0.001
regularization_loss = 0
for param in model.parameters():
    regularization_loss += torch.norm(param, p=2)  # L2 regularization term



def NMSELoss(input, target):
    
    # Calculate squared error
    squared_error = (input - target) ** 2
        
    # Calculate mean squared error
    mse = torch.mean(squared_error)

    # Calculate normalized mean squared error
    # Divide mean squared error by the variance of the target
    # This helps in normalizing the loss across different scales of the target
    nmse = mse / torch.var(target)

    return nmse

num_epochs = 5
#device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device = torch.device("cuda")
pck_50_overall_max = 0
train_mean_loss_iter = []
valid_mean_loss_iter = []
time_iter = []

for epoch_index in range(num_epochs):

    loss = 0
    train_loss_iter = []
    metric = []
    dis.train()
    model.train()
    relation_mean =[]
    dis_real_list = []
    dis_fake_list = []
    dis_prior_list = []
    relation_mean =[]
    gan_loss_list = []
    recon_loss_list = []
    prior_loss_list = []
    for idx, data in enumerate(train_loader):

        csi_data = data['input_wifi-csi']
        bs = 32
  
        csi_data = torch.tensor(csi_data)
        csi_data = csi_data.cuda()
        csi_data = csi_data.type(torch.cuda.FloatTensor)
        #csi_dafeaturesta = csi_data.view(16,2,3,114,10)
        keypoint = data['output']#17,3
        keypoint = keypoint.cuda()
       
        
        xy_keypoint = keypoint[:,:,0:2].cuda()
        confidence = keypoint[:,:,2:3].cuda()
        
        
        # Discriminator update 
        optimizer_GAN.zero_grad()
        #optimizer.zero_grad()

        #training with real CSI
        ones_label=torch.ones(bs,1).to(device)
        output_real = dis(csi_data)[0]
        errD_real = criterion1(output_real, ones_label)
        errD_real.backward()

        
        #training with the rec_CSI    
        fake_csi = model(csi_data)[1]    
        zeros_label=torch.zeros(bs,1).to(device)
        output_fake = dis(fake_csi)[0]
        errD_fake = criterion1(output_fake, zeros_label)
        errD_fake.backward()
        
        #training adv CSI
        epsilon = 0.1
        num_iterations = 1
        alpha = epsilon/num_iterations
        csi_data.requires_grad = True
        for _ in range(num_iterations):
            # Forward pass
            outputs = model(csi_data)[1]

            # Calculate the loss
            #loss = criterion2(torch.mul(confidence, outputs), torch.mul(confidence, xy_keypoint)) / 32
            loss = NMSELoss(csi_data,outputs)

            # Zero all existing gradients
            optimizer.zero_grad()

            # Calculate gradients of model in backward pass
            loss.backward()

            # Collect datagrad
            data_grad = csi_data.grad.data

            # Collect the element-wise sign of the data gradient
            sign_data_grad = data_grad.sign()

            # Create the perturbed image by adjusting each pixel of the input image
            
            perturbed_csi_data = csi_data + alpha * sign_data_grad

            # Clip the perturbation to ensure it stays within the epsilon ball
            #perturbed_csi_data = torch.clamp(perturbed_csi_data, 0, 1)

            # Update the input for the next iteration
            csi_data.data = perturbed_csi_data.detach().clone().requires_grad_(True)

        
        #optimizer_GAN.zero_grad()
        
        adv_csi = model(perturbed_csi_data)[1]    
        zeros_label=torch.zeros(bs,1).to(device)
        output_adv = dis(adv_csi)[0]
        errD_adv = criterion1(output_adv, zeros_label)
        errD_adv.backward() 

        #update discriminator

        optimizer_GAN.step()
        #optimizer.step()
        gan_loss = errD_real + errD_fake + errD_adv
        #loss_rec = NMSELoss(csi_data, fake_csi)
        
        #Generator and VQ-VAE update 
        
        optimizer.zero_grad()
        
        #forward pass through VQ-VAE
        
        
        
        
        #Reconstruction loss
        dec_csi = model(csi_data)[1]
        #adv_csi = model(perturbed_csi_data)[1]
        rec_loss = criterion2(dec_csi, csi_data) 
        
        # Fooling the discriminator loss
        output = dis(dec_csi)[0]
        #gen_loss = criterion1(output, ones_label)
        
        vq_loss, recover_signal, pred_xy_keypoint = model(csi_data)
        recover_signal = recover_signal.to(device)
        pred_xy_keypoint = pred_xy_keypoint.to(device)
        keypoint = keypoint.to(device)
        loss_keypoint = criterion2(torch.mul(confidence, pred_xy_keypoint), torch.mul(confidence, xy_keypoint))/32
        
        
        # Combined loss
        loss = rec_loss  + vq_loss + loss_keypoint 
        loss.backward()
        
        # Update generator and VQ-VAE
        optimizer.step()
        
                
        
    

       
       
       
       

        

        lr = np.array(scheduler.get_last_lr())
        #print(f"FLOPs: {flops / 1e9} GigaFLOPs")  # Chuyển đổi sang GigaFLOPs (tỷ lệ FLOPs)
        #print(f"Parameters: {params / 1e6} Million")
        message = '(epoch: %d, iters: %d, lr: %.5f, loss: %.3f, loss_rec: %.3f) ' % (epoch_index, idx * 32, lr,loss_keypoint, rec_loss)
        print(message)
    scheduler.step()
    sum_time = np.mean(time_iter)
    train_mean_loss = np.mean(train_loss_iter)
    train_mean_loss_iter.append(train_mean_loss)
    #relation_mean = np.mean(relation, 0)
    print('end of the epoch: %d, with loss: %.3f' % (epoch_index, train_mean_loss,))
    #total_params = sum(p.numel() for p in metafi.parameters())
    #print("Số lượng tham số trong mô hình: ", total_params)
    #print("Tổng thời gian train: ", sum_time)
    
    dis.eval()
    model.eval()
    valid_loss_total_iter = []
    valid_loss_perform_iter = []
    valid_loss_recover_iter = []
    #metric = []
    pck_50_iter = []
    pck_20_iter = []
    with torch.no_grad():
        for idx, data in enumerate(val_loader):
            csi_data = data['input_wifi-csi']
            
            
            
            csi_data = torch.tensor(csi_data)
            csi_data = csi_data.cuda()
            csi_data = csi_data.type(torch.cuda.FloatTensor)
            
            #csi_dafeaturesta = csi_data.view(16,2,3,114,10)
            keypoint = data['output']#17,3
            keypoint = keypoint.cuda()
            
            
            xy_keypoint = keypoint[:,:,0:2].cuda()
            confidence = keypoint[:,:,2:3].cuda()

            vq_loss, recover_signal, pred_xy_keypoint = model(csi_data)
            recover_signal = recover_signal.to(device)
            pred_xy_keypoint = pred_xy_keypoint.to(device)
            #pred_xy_keypoint = pred_xy_keypoint.type(torch.FloatTensor)
            keypoint = keypoint.to(device)
            keypoint = keypoint.type(torch.LongTensor)
            loss1 = criterion2(torch.mul(confidence, pred_xy_keypoint), torch.mul(confidence, xy_keypoint))/32
            loss2 = NMSELoss(recover_signal,csi_data)
            loss = loss1 + loss2 + vq_loss 
            
            #loss = tf.reduce_mean(tf.pow(pred_xy_keypoint - xy_keypoint, 2))
            #loss = criterion_L2(torch.mul(confidence, pred_xy_keypoint), torch.mul(confidence, xy_keypoint))
            #loss = criterion_L2(pred_xy_keypoint, xy_keypoint)
            
            valid_loss_total_iter.append(loss.cpu().detach().numpy())
            valid_loss_perform_iter.append(loss1.cpu().detach().numpy())
            valid_loss_recover_iter.append(loss2.cpu().detach().numpy())
            pred_xy_keypoint = pred_xy_keypoint.cpu()
            xy_keypoint = xy_keypoint.cpu()
            #pred_xy_keypoint = torch.transpose(pred_xy_keypoint, 0, 1).unsqueeze(dim=0)
            #xy_keypoint = torch.transpose(xy_keypoint, 0, 1).unsqueeze(dim=0)
            pred_xy_keypoint_pck = torch.transpose(pred_xy_keypoint, 1, 2)
            xy_keypoint_pck = torch.transpose(xy_keypoint, 1, 2)
            #keypoint = torch.transpose(keypoint, 1, 2)
            #pred_xy_keypoint_pck = pred_xy_keypoint.cpu()
            #xy_keypoint_pck = xy_keypoint.cpu()
            pck = compute_pck_pckh(pred_xy_keypoint_pck, xy_keypoint_pck, 0.5)
            #mpjpe,pa_mpjpe = calulate_error(pred_xy_keypoint, xy_keypoint)
             
            metric.append(calulate_error(pred_xy_keypoint, xy_keypoint))
            pck_50_iter.append(compute_pck_pckh(pred_xy_keypoint_pck, xy_keypoint_pck, 0.5))
            pck_20_iter.append(compute_pck_pckh(pred_xy_keypoint_pck, xy_keypoint_pck, 0.2))
            
            
            #message1 = '( loss: %.3f) ' % (loss)
            #print(message1)
            #pck_50_iter.append(compute_pck_pckh(pred_xy_keypoint, xy_keypoint, 0.5))
            #print(f"FLOPs: {flops / 1e9} GigaFLOPs")  # Chuyển đổi sang GigaFLOPs (tỷ lệ FLOPs)
            #print(f"Parameters: {params / 1e6} Million")
            #pck_20_iter.append(compute_pck_pckh(pred_xy_keypoint, xy_keypoint, 0.2))

        valid_mean_total_loss = np.mean(valid_loss_total_iter)
        valid_mean_perform_loss = np.mean(valid_loss_perform_iter)
        valid_mean_recover_loss = np.mean(valid_loss_recover_iter)
        #train_mean_loss = np.mean(train_loss_iter)
        valid_mean_loss_iter.append(valid_mean_total_loss)
        mean = np.mean(metric, 0)*1000
        mpjpe_mean = mean[0]
        pa_mpjpe_mean = mean[1]
        pck_50 = np.mean(pck_50_iter,0)
        pck_20 = np.mean(pck_20_iter,0)
        pck_50_overall = pck_50[17]
        pck_20_overall = pck_20[17]
        print('loss_total: %.3f,loss_perform: %.3f,loss_recover: %.3f, pck_50: %.3f, pck_20: %.3f, mpjpe: %.3f, pa_mpjpe: %.3f' % (valid_mean_total_loss,valid_mean_perform_loss,valid_mean_recover_loss, pck_50_overall,pck_20_overall, mpjpe_mean, pa_mpjpe_mean))
        
        if pck_50_overall > pck_50_overall_max:
           print('saving the model at the end of epoch %d with pck_50: %.3f' % (epoch_index, pck_50_overall))
           torch.save(model, 'get_train.py')
           pck_50_overall_max = pck_50_overall


        if (epoch_index+1) % 50 == 0:
            print('the train loss for the first %.1f epoch is' % (epoch_index))
            print(train_mean_loss_iter)


import matplotlib.pyplot as plt

epochs = list(range(1,num_epochs+1))
training_loss = train_mean_loss_iter
validation_loss = valid_mean_loss_iter

plt.plot(epochs, training_loss, label='Training Loss', color='blue')

# Vẽ đồ thị loss function cho tập validation
plt.plot(epochs, validation_loss, label='Validation Loss', color='red')

# Tùy chỉnh đồ thị
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Đồ thị Loss Function qua Epochs')
plt.legend()
# Hiển thị đồ thị
plt.show()             


        


        







